{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MILESTONE TWO: Model Development, Evaluation, and Deployment (PART B)\n",
    "\n",
    "**Course**: DSC8201 - Data Science Lifecycle  \n",
    "**Project**: Financial Credit Scoring & Fairness Auditing  \n",
    "**Student**: Atuhaire (B35093)  \n",
    "**Date**: December 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Model Selection & Justification](#section1)\n",
    "2. [Model Development & Experiment Tracking](#section2)\n",
    "3. [Model Evaluation & Interpretation](#section3)\n",
    "4. [Fairness Analysis](#section4)\n",
    "5. [Summary](#section5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report, roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "# Classical ML Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Imbalanced data handling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Deep Learning (optional)\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    TF_AVAILABLE = True\n",
    "except:\n",
    "    TF_AVAILABLE = False\n",
    "    print(\"TensorFlow not available, skipping deep learning models\")\n",
    "\n",
    "# Model Explainability\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print(\" SHAP not available\")\n",
    "\n",
    "try:\n",
    "    from lime.lime_tabular import LimeTabularExplainer\n",
    "    LIME_AVAILABLE = True\n",
    "except:\n",
    "    LIME_AVAILABLE = False\n",
    "    print(\" LIME not available\")\n",
    "\n",
    "# MLflow for experiment tracking\n",
    "try:\n",
    "    import mlflow\n",
    "    import mlflow.sklearn\n",
    "    MLFLOW_AVAILABLE = True\n",
    "except:\n",
    "    MLFLOW_AVAILABLE = False\n",
    "    print(\" MLflow not available\")\n",
    "\n",
    "# Fairness libraries\n",
    "try:\n",
    "    from fairlearn.metrics import MetricFrame, selection_rate, demographic_parity_difference\n",
    "    from fairlearn.reductions import ExponentiatedGradient, DemographicParity\n",
    "    FAIRLEARN_AVAILABLE = True\n",
    "except:\n",
    "    FAIRLEARN_AVAILABLE = False\n",
    "    print(\" Fairlearn not available\")\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "from utils import *\n",
    "from preprocessing import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set options\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\" Libraries imported successfully!\")\n",
    "print(f\"Working Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"LOADING PREPROCESSED DATA\")\n",
    "\n",
    "# Load the clean dataset\n",
    "data_path = Path.cwd().parent / 'data' / 'cleaned' / 'Atuhaire.csv'\n",
    "\n",
    "if data_path.exists():\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\" Dataset loaded successfully!\")\n",
    "    print(f\"   Shape: {df.shape}\")\n",
    "    print(f\"   File: {data_path}\")\n",
    "else:\n",
    "    print(f\" Dataset not found at {data_path}\")\n",
    "    print(\"   Please run the data preparation notebook first!\")\n",
    "    raise FileNotFoundError(f\"Dataset not found: {data_path}\")\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(f\"  Columns: {len(df.columns)}\")\n",
    "print(f\"  Rows: {len(df):,}\")\n",
    "print(f\"  Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section1'></a>\n",
    "## 1. Model Selection & Justification [8 Marks]\n",
    "\n",
    "### 1.1 Model Categories & Selection Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"MODEL SELECTION FRAMEWORK\")\n",
    "\n",
    "print(\"\"\"\n",
    " MODEL SELECTION STRATEGY FOR CREDIT SCORING:\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "SELECTION CRITERIA:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "1. HYPOTHESIS ALIGNMENT:\n",
    "   • H₁: Financial attributes predict credit default\n",
    "   • Need models that capture both linear and non-linear relationships\n",
    "   • Require probability outputs for risk scoring\n",
    "\n",
    "2. DATA CHARACTERISTICS:\n",
    "   • Size: 40,000 samples (medium-sized dataset)\n",
    "   • Features: ~50-60 after encoding (tabular data)\n",
    "   • Class Imbalance: ~15-20% default rate (will use SMOTE)\n",
    "   • Mixed feature types: Numerical + Categorical (encoded)\n",
    "\n",
    "3. ETHICAL & REGULATORY CONSIDERATIONS:\n",
    "   • Model must be interpretable (regulatory requirement)\n",
    "   • Need to detect and mitigate bias\n",
    "   • Explainability critical for loan rejections\n",
    "   • Must comply with fair lending laws\n",
    "\n",
    "4. INTERPRETABILITY REQUIREMENTS:\n",
    "   • High: Regulatory compliance, customer explanations\n",
    "   • Use SHAP/LIME for complex models\n",
    "   • Feature importance must be calculable\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "SELECTED MODELS:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    " CATEGORY 1: CLASSICAL MACHINE LEARNING\n",
    "\n",
    "   1. Logistic Regression\n",
    "      ✓ Highly interpretable (coefficients = feature importance)\n",
    "      ✓ Fast training and prediction\n",
    "      ✓ Probability calibration built-in\n",
    "      ✓ Industry standard for credit scoring\n",
    "      ✗ Assumes linear relationships\n",
    "      → Use as baseline model\n",
    "   \n",
    "   2. Random Forest Classifier\n",
    "      ✓ Handles non-linear relationships\n",
    "      ✓ Feature importance via Gini/entropy\n",
    "      ✓ Robust to outliers\n",
    "      ✓ No scaling required\n",
    "      ✗ Less interpretable than logistic regression\n",
    "      → Good for ensemble and feature selection\n",
    "   \n",
    "   3. XGBoost (Gradient Boosting)\n",
    "      ✓ State-of-the-art performance on tabular data\n",
    "      ✓ Handles class imbalance well (scale_pos_weight)\n",
    "      ✓ Feature importance available\n",
    "      ✓ Regularization prevents overfitting\n",
    "      ✓ SHAP integration\n",
    "      → Expected best performer\n",
    "   \n",
    "   4. LightGBM\n",
    "      ✓ Faster than XGBoost on large datasets\n",
    "      ✓ Handles categorical features natively\n",
    "      ✓ Memory efficient\n",
    "      ✓ Similar performance to XGBoost\n",
    "      → Alternative to XGBoost\n",
    "\n",
    " CATEGORY 2: DEEP LEARNING (Optional)\n",
    "\n",
    "   5. Deep Neural Network (DNN)\n",
    "      ✓ Can learn complex patterns\n",
    "      ✓ Automatic feature interaction learning\n",
    "      ✗ Requires more data for optimal performance\n",
    "      ✗ Less interpretable (use SHAP)\n",
    "      ✗ Longer training time\n",
    "      → Include if performance gains are significant\n",
    "\n",
    " CATEGORY 3: FAIRNESS-AWARE ML\n",
    "\n",
    "   6. Fairness-Constrained Classifier\n",
    "      ✓ Explicitly optimizes for fairness metrics\n",
    "      ✓ Uses Exponentiated Gradient method\n",
    "      ✓ Addresses demographic parity\n",
    "      ✗ May sacrifice some accuracy for fairness\n",
    "      → Critical for compliance\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "JUSTIFICATION SUMMARY:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    " HYPOTHESIS: Multiple models test different relationships (linear vs non-linear)\n",
    " DATA SIZE: 40K samples suitable for both classical ML and shallow DNNs\n",
    " ETHICS: Fairness-aware models + SHAP explainability ensure compliance\n",
    " INTERPRETABILITY: Mix of transparent (LogReg) and explainable (XGB+SHAP) models\n",
    "\n",
    "Final Model Selection: Will be based on:\n",
    "  1. Predictive Performance (AUC-ROC, F1-score)\n",
    "  2. Fairness Metrics (Disparate Impact Ratio)\n",
    "  3. Interpretability (SHAP values, feature importance)\n",
    "  4. Business Requirements (speed, explainability)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"DATA PREPARATION FOR MODELING\")\n",
    "\n",
    "# Separate features and target\n",
    "# Exclude sensitive attributes from features (fairness requirement)\n",
    "sensitive_features = ['gender', 'age_group', 'marital_status']  # Keep for fairness analysis only\n",
    "\n",
    "# Identify target\n",
    "target_col = 'default_status'\n",
    "\n",
    "# Get all column names\n",
    "all_cols = df.columns.tolist()\n",
    "\n",
    "# Features to exclude from modeling\n",
    "exclude_cols = [target_col, 'applicant_id'] + [col for col in sensitive_features if col in all_cols]\n",
    "\n",
    "# Select features\n",
    "feature_cols = [col for col in all_cols if col not in exclude_cols]\n",
    "\n",
    "print(f\"\\n Feature Selection:\")\n",
    "print(f\"   Total columns: {len(all_cols)}\")\n",
    "print(f\"   Target: {target_col}\")\n",
    "print(f\"   Features for modeling: {len(feature_cols)}\")\n",
    "print(f\"   Excluded (sensitive/ID): {len(exclude_cols)}\")\n",
    "\n",
    "# Create X and y\n",
    "X = df[feature_cols].copy()\n",
    "y = df[target_col].copy()\n",
    "\n",
    "# Store sensitive attributes separately for fairness analysis\n",
    "sensitive_attrs = df[[col for col in sensitive_features if col in df.columns]].copy()\n",
    "\n",
    "print(f\"\\n Data prepared:\")\n",
    "print(f\"   X shape: {X.shape}\")\n",
    "print(f\"   y shape: {y.shape}\")\n",
    "print(f\"   Class distribution: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test-Validation Split\n",
    "print(\"\\n Creating Train-Validation-Test Split...\\n\")\n",
    "\n",
    "# First split: 80% train+val, 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: 75% train, 25% validation (of the 80%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# Also split sensitive attributes for fairness analysis\n",
    "if len(sensitive_attrs) > 0:\n",
    "    sensitive_temp, sensitive_test = train_test_split(\n",
    "        sensitive_attrs, test_size=0.20, random_state=42, stratify=y\n",
    "    )\n",
    "    sensitive_train, sensitive_val = train_test_split(\n",
    "        sensitive_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    "    )\n",
    "\n",
    "print(f\"Train Set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  - Default rate: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"\\nValidation Set: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  - Default rate: {y_val.mean()*100:.2f}%\")\n",
    "print(f\"\\nTest Set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  - Default rate: {y_test.mean()*100:.2f}%\")\n",
    "print(f\"\\n Stratified split ensures consistent class distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Address Class Imbalance with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"HANDLING CLASS IMBALANCE\")\n",
    "\n",
    "print(\"\\n Original Class Distribution:\")\n",
    "print(f\"   No Default (0): {(y_train == 0).sum():,} ({(y_train == 0).mean()*100:.2f}%)\")\n",
    "print(f\"   Default (1): {(y_train == 1).sum():,} ({(y_train == 1).mean()*100:.2f}%)\")\n",
    "print(f\"   Imbalance Ratio: {(y_train == 0).sum() / (y_train == 1).sum():.2f}:1\")\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "smote = SMOTE(random_state=42, sampling_strategy=0.7)  # Increase minority class to 70% of majority\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\n After SMOTE Resampling:\")\n",
    "print(f\"   No Default (0): {(y_train_balanced == 0).sum():,} ({(y_train_balanced == 0).mean()*100:.2f}%)\")\n",
    "print(f\"   Default (1): {(y_train_balanced == 1).sum():,} ({(y_train_balanced == 1).mean()*100:.2f}%)\")\n",
    "print(f\"   New Imbalance Ratio: {(y_train_balanced == 0).sum() / (y_train_balanced == 1).sum():.2f}:1\")\n",
    "print(f\"\\nTraining data size increased from {len(X_train):,} to {len(X_train_balanced):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Continued in next cells...**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
