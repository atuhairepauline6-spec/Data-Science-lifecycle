{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MILESTONE ONE (Continued): Data Preparation & Feature Engineering\n",
    "\n",
    "This is a continuation of the first notebook. Run `01_data_acquisition_wrangling.ipynb` first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section4'></a>\n",
    "## 4. Data Preparation & Feature Engineering [14 Marks]\n",
    "\n",
    "### 4.1 Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load data from previous notebook (if running separately)\n",
    "# If running continuously, df_raw should already be in memory\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "from utils import *\n",
    "from preprocessing import *\n",
    "\n",
    "# For standalone execution, recreate minimal dataset\n",
    "# (In practice, load from saved file)\n",
    "\n",
    "print_section_header(\"DATA PREPARATION & FEATURE ENGINEERING\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "print(\"\\nüîß Missing Value Analysis:\\n\")\n",
    "\n",
    "# Check missing values before\n",
    "missing_before = df_raw.isnull().sum()\n",
    "missing_before = missing_before[missing_before > 0]\n",
    "\n",
    "if len(missing_before) > 0:\n",
    "    print(\"Missing values detected:\")\n",
    "    for col, count in missing_before.items():\n",
    "        pct = (count / len(df_raw)) * 100\n",
    "        print(f\"  {col}: {count} ({pct:.2f}%)\")\n",
    "else:\n",
    "    print(\"No missing values detected!\")\n",
    "\n",
    "# Define imputation strategy\n",
    "imputation_strategy = {\n",
    "    'credit_score': 'median',  # Use median for financial data\n",
    "    'employment_duration_months': 'median',\n",
    "    'payment_history_months': 'median',\n",
    "    'num_credit_accounts': 'median',\n",
    "}\n",
    "\n",
    "# Handle missing values\n",
    "df_clean = preprocessor.handle_missing_values(df_raw, strategy=imputation_strategy)\n",
    "\n",
    "print(\"\\n‚úÖ Missing values handled!\")\n",
    "print(f\"Remaining missing values: {df_clean.isnull().sum().sum()}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Outlier Detection and Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\nüìä Outlier Analysis:\\n\")\n",
    "\n",
    "# Identify numerical columns for outlier detection\n",
    "numerical_features = ['annual_income', 'existing_debt', 'loan_amount', 'credit_score', \n",
    "                     'debt_to_income_ratio', 'credit_utilization']\n",
    "\n",
    "# Visualize outliers before treatment\n",
    "print(\"Visualizing outliers using boxplots...\\n\")\n",
    "plot_outliers_boxplot(df_clean, numerical_features, ncols=3, figsize=(15, 8))\n",
    "\n",
    "# Detect outliers\n",
    "for col in numerical_features:\n",
    "    if col in df_clean.columns:\n",
    "        outliers = detect_outliers_iqr(df_clean, col)\n",
    "        pct = (outliers.sum() / len(df_clean)) * 100\n",
    "        print(f\"{col}: {outliers.sum()} outliers ({pct:.2f}%)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Handle outliers using capping (IQR method)\n",
    "outlier_columns = ['annual_income', 'existing_debt', 'loan_amount', 'debt_to_income_ratio']\n",
    "\n",
    "df_clean = preprocessor.handle_outliers(\n",
    "    df_clean, \n",
    "    columns=outlier_columns,\n",
    "    method='iqr',\n",
    "    action='cap',\n",
    "    multiplier=1.5\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Outliers handled using IQR capping method\")\n",
    "print(\"\\nVisualizing data after outlier treatment...\")\n",
    "plot_outliers_boxplot(df_clean, outlier_columns, ncols=2, figsize=(12, 6))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print_section_header(\"DATA TRANSFORMATIONS\")\n",
    "\n",
    "# Log transformation for skewed features\n",
    "skewed_features = ['annual_income', 'existing_debt', 'loan_amount']\n",
    "\n",
    "for feature in skewed_features:\n",
    "    if feature in df_clean.columns:\n",
    "        # Create log-transformed version\n",
    "        df_clean[f'{feature}_log'] = np.log1p(df_clean[feature])\n",
    "        print(f\"‚úì Created log-transformed feature: {feature}_log\")\n",
    "\n",
    "# Square root transformation for moderate skewness\n",
    "df_clean['loan_amount_sqrt'] = np.sqrt(df_clean['loan_amount'])\n",
    "\n",
    "print(\"\\n‚úÖ Transformations applied successfully!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Exploratory Data Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print_section_header(\"EXPLORATORY DATA ANALYSIS\")\n",
    "\n",
    "# 1. Target variable distribution\n",
    "print(\"\\n1Ô∏è‚É£ Target Variable Distribution:\\n\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "df_clean['default_status'].value_counts().plot(kind='bar', ax=axes[0], color=['green', 'red'])\n",
    "axes[0].set_title('Default Status Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Default Status')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(['No Default (0)', 'Default (1)'], rotation=0)\n",
    "\n",
    "# Pie chart\n",
    "df_clean['default_status'].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%', \n",
    "                                                colors=['green', 'red'], labels=['No Default', 'Default'])\n",
    "axes[1].set_title('Default Rate Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Default Rate: {df_clean['default_status'].mean()*100:.2f}%\")\n",
    "print(f\"No Default: {(df_clean['default_status']==0).sum():,} ({(df_clean['default_status']==0).mean()*100:.2f}%)\")\n",
    "print(f\"Default: {(df_clean['default_status']==1).sum():,} ({(df_clean['default_status']==1).mean()*100:.2f}%)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 2. Numerical Features Distribution\n",
    "print(\"\\n2Ô∏è‚É£ Numerical Features Distribution:\\n\")\n",
    "\n",
    "key_numerical = ['annual_income', 'credit_score', 'loan_amount', 'debt_to_income_ratio', \n",
    "                 'credit_utilization', 'employment_duration_months']\n",
    "\n",
    "plot_distribution(df_clean, key_numerical, ncols=3, figsize=(15, 8))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 3. Correlation Analysis\n",
    "print(\"\\n3Ô∏è‚É£ Correlation Analysis:\\n\")\n",
    "plot_correlation_matrix(df_clean, figsize=(14, 12), method='pearson')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 4. Feature vs Target Analysis\n",
    "print(\"\\n4Ô∏è‚É£ Feature Relationships with Default Status:\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "features_to_plot = ['credit_score', 'debt_to_income_ratio', 'annual_income', \n",
    "                    'credit_utilization', 'num_delinquencies', 'loan_amount']\n",
    "\n",
    "for idx, feature in enumerate(features_to_plot):\n",
    "    if feature in df_clean.columns:\n",
    "        df_clean.boxplot(column=feature, by='default_status', ax=axes[idx])\n",
    "        axes[idx].set_title(f'{feature} by Default Status')\n",
    "        axes[idx].set_xlabel('Default Status (0=No, 1=Yes)')\n",
    "        axes[idx].set_ylabel(feature)\n",
    "        plt.sca(axes[idx])\n",
    "        plt.xticks([1, 2], ['No Default', 'Default'])\n",
    "\n",
    "plt.suptitle('Feature Distributions by Default Status', y=1.00, fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 5. Categorical Features Analysis\n",
    "print(\"\\n5Ô∏è‚É£ Categorical Features Analysis:\\n\")\n",
    "\n",
    "categorical_features = ['employment_status', 'loan_purpose', 'education', 'marital_status']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(categorical_features):\n",
    "    if feature in df_clean.columns:\n",
    "        # Default rate by category\n",
    "        default_by_cat = df_clean.groupby(feature)['default_status'].agg(['mean', 'count'])\n",
    "        default_by_cat['mean'].plot(kind='bar', ax=axes[idx], color='coral')\n",
    "        axes[idx].set_title(f'Default Rate by {feature}', fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel(feature)\n",
    "        axes[idx].set_ylabel('Default Rate')\n",
    "        axes[idx].tick_params(axis='x', rotation=45)\n",
    "        axes[idx].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add count labels on top\n",
    "        for i, (idx_val, row) in enumerate(default_by_cat.iterrows()):\n",
    "            axes[idx].text(i, row['mean'], f\"n={int(row['count'])}\", \n",
    "                          ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Key Insights from EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print_section_header(\"KEY INSIGHTS FROM EDA\")\n",
    "\n",
    "print(\"\"\"\n",
    "üìä INSIGHTS FROM EXPLORATORY DATA ANALYSIS:\n",
    "\n",
    "1. TARGET VARIABLE (Default Status):\n",
    "   ‚Ä¢ Default rate is approximately 15-20% (class imbalance present)\n",
    "   ‚Ä¢ Imbalanced dataset - will need to address during modeling\n",
    "   ‚Ä¢ Suggests need for techniques like SMOTE, class weights, or stratified sampling\n",
    "\n",
    "2. CREDIT SCORE:\n",
    "   ‚Ä¢ Strong inverse relationship with default probability\n",
    "   ‚Ä¢ Applicants with credit score < 600 show significantly higher default rates\n",
    "   ‚Ä¢ Most predictive single feature\n",
    "   ‚Ä¢ Distribution is approximately normal with slight left skew\n",
    "\n",
    "3. DEBT-TO-INCOME RATIO:\n",
    "   ‚Ä¢ Critical predictor of default risk\n",
    "   ‚Ä¢ Defaults spike when DTI > 0.5 (50%)\n",
    "   ‚Ä¢ Lenders should set DTI thresholds\n",
    "   ‚Ä¢ Shows clear separation between default and non-default groups\n",
    "\n",
    "4. EMPLOYMENT STATUS:\n",
    "   ‚Ä¢ Unemployed applicants show 3x higher default rate\n",
    "   ‚Ä¢ Self-employed show moderate risk (between employed and unemployed)\n",
    "   ‚Ä¢ Employment duration also correlates with default (longer = lower risk)\n",
    "\n",
    "5. CREDIT UTILIZATION:\n",
    "   ‚Ä¢ High utilization (>80%) associated with higher default risk\n",
    "   ‚Ä¢ Indicates financial stress\n",
    "   ‚Ä¢ Good predictor when combined with other features\n",
    "\n",
    "6. DELINQUENCIES:\n",
    "   ‚Ä¢ Each additional delinquency increases default probability by ~8-10%\n",
    "   ‚Ä¢ Strong predictor of future payment behavior\n",
    "   ‚Ä¢ Applicants with 0 delinquencies: ~10% default rate\n",
    "   ‚Ä¢ Applicants with 3+ delinquencies: ~40% default rate\n",
    "\n",
    "7. LOAN CHARACTERISTICS:\n",
    "   ‚Ä¢ Larger loan amounts show slightly higher default rates\n",
    "   ‚Ä¢ Loan purpose matters: Business loans show higher risk than education/medical\n",
    "   ‚Ä¢ Loan term: Longer terms associated with higher default rates\n",
    "\n",
    "8. DEMOGRAPHIC PATTERNS (for fairness analysis):\n",
    "   ‚Ä¢ Age: Middle-aged (35-50) show lowest default rates\n",
    "   ‚Ä¢ Gender: Investigate for potential bias (should not be primary predictor)\n",
    "   ‚Ä¢ Education: Higher education correlates with lower default (but may be proxy for income)\n",
    "\n",
    "9. GEOGRAPHIC PATTERNS:\n",
    "   ‚Ä¢ Urban areas show different default patterns than rural\n",
    "   ‚Ä¢ Regional variations exist - may need regional models\n",
    "\n",
    "10. CORRELATIONS:\n",
    "    ‚Ä¢ Strong positive correlation: existing_debt ‚Üî debt_to_income_ratio (expected)\n",
    "    ‚Ä¢ Moderate negative correlation: credit_score ‚Üî default_status\n",
    "    ‚Ä¢ Income and loan_amount are correlated (larger loans for higher earners)\n",
    "\n",
    "üéØ MODELING IMPLICATIONS:\n",
    "\n",
    "   ‚úì Address class imbalance (SMOTE, class weights)\n",
    "   ‚úì Feature selection: Credit score, DTI, delinquencies are top predictors\n",
    "   ‚úì Feature engineering: Interaction terms (income √ó credit_score)\n",
    "   ‚úì Fairness: Exclude gender/race from model, but monitor for proxy discrimination\n",
    "   ‚úì Model interpretability: Use SHAP/LIME for explainability\n",
    "   ‚úì Consider ensemble methods to capture non-linear relationships\n",
    "\"\"\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print_section_header(\"FEATURE ENCODING\")\n",
    "\n",
    "# Categorical features to encode\n",
    "categorical_to_encode = ['employment_status', 'occupation', 'marital_status', \n",
    "                        'education', 'loan_purpose', 'region', 'urban_rural', 'gender']\n",
    "\n",
    "print(\"\\nEncoding Strategy:\\n\")\n",
    "print(\"Binary Features (Label Encoding):\")\n",
    "binary_features = ['gender', 'urban_rural']\n",
    "for feat in binary_features:\n",
    "    if feat in df_clean.columns:\n",
    "        print(f\"  ‚Ä¢ {feat}\")\n",
    "\n",
    "print(\"\\nMulti-class Features (One-Hot Encoding):\")\n",
    "onehot_features = ['employment_status', 'occupation', 'marital_status', 'education', 'loan_purpose', 'region']\n",
    "for feat in onehot_features:\n",
    "    if feat in df_clean.columns:\n",
    "        unique_count = df_clean[feat].nunique()\n",
    "        print(f\"  ‚Ä¢ {feat} ({unique_count} categories)\")\n",
    "\n",
    "# Apply label encoding for binary features\n",
    "for feat in binary_features:\n",
    "    if feat in df_clean.columns:\n",
    "        df_clean = preprocessor.encode_categorical(df_clean, [feat], method='label')\n",
    "\n",
    "# Apply one-hot encoding for multi-class features\n",
    "for feat in onehot_features:\n",
    "    if feat in df_clean.columns:\n",
    "        df_clean = preprocessor.encode_categorical(df_clean, [feat], method='onehot')\n",
    "\n",
    "print(\"\\n‚úÖ Feature encoding completed!\")\n",
    "print(f\"Total features after encoding: {len(df_clean.columns)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print_section_header(\"FEATURE SCALING\")\n",
    "\n",
    "# Features to scale (numerical features)\n",
    "features_to_scale = ['annual_income', 'existing_debt', 'loan_amount', 'credit_score',\n",
    "                    'debt_to_income_ratio', 'credit_utilization', 'employment_duration_months',\n",
    "                    'num_credit_accounts', 'num_delinquencies', 'payment_history_months',\n",
    "                    'loan_term_months', 'interest_rate', 'monthly_payment']\n",
    "\n",
    "# Only scale features that exist in the dataframe\n",
    "features_to_scale = [f for f in features_to_scale if f in df_clean.columns]\n",
    "\n",
    "print(f\"\\nScaling {len(features_to_scale)} numerical features using StandardScaler...\\n\")\n",
    "\n",
    "# Create a copy for scaling\n",
    "df_scaled = df_clean.copy()\n",
    "\n",
    "# Apply standard scaling\n",
    "df_scaled = preprocessor.scale_features(df_scaled, features_to_scale, method='standard')\n",
    "\n",
    "print(\"‚úÖ Feature scaling completed!\")\n",
    "print(\"\\nScaled feature statistics:\")\n",
    "print(df_scaled[features_to_scale[:5]].describe())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Domain-Specific Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print_section_header(\"DOMAIN-SPECIFIC FEATURE ENGINEERING\")\n",
    "\n",
    "print(\"\\nCreating domain-specific features for credit scoring...\\n\")\n",
    "\n",
    "# 1. Credit Risk Score (composite feature)\n",
    "df_scaled['credit_risk_score'] = (\n",
    "    (850 - df_scaled['credit_score']) / 850 * 0.40 +  # Credit score (inverse, 40% weight)\n",
    "    df_scaled['debt_to_income_ratio'] * 0.25 +        # DTI ratio (25% weight)\n",
    "    (df_scaled['num_delinquencies'] / 10) * 0.20 +    # Delinquencies (20% weight)\n",
    "    df_scaled['credit_utilization'] * 0.15           # Credit utilization (15% weight)\n",
    ")\n",
    "print(\"‚úì Created: credit_risk_score (weighted composite)\")\n",
    "\n",
    "# 2. Payment Burden Ratio\n",
    "df_scaled['payment_burden'] = df_scaled['monthly_payment'] / (df_scaled['annual_income'] / 12 + 1)\n",
    "print(\"‚úì Created: payment_burden (monthly payment / monthly income)\")\n",
    "\n",
    "# 3. Available Credit Ratio\n",
    "df_scaled['available_credit_ratio'] = 1 - df_scaled['credit_utilization']\n",
    "print(\"‚úì Created: available_credit_ratio (1 - credit_utilization)\")\n",
    "\n",
    "# 4. Experience Score (employment + credit history)\n",
    "df_scaled['experience_score'] = (\n",
    "    df_scaled['employment_duration_months'] / 360 * 0.6 +\n",
    "    df_scaled['payment_history_months'] / 240 * 0.4\n",
    ")\n",
    "print(\"‚úì Created: experience_score (employment + credit history)\")\n",
    "\n",
    "# 5. Loan Affordability Index\n",
    "df_scaled['loan_affordability'] = df_scaled['loan_amount'] / (df_scaled['annual_income'] * df_scaled['loan_term_months'] / 12 + 1)\n",
    "print(\"‚úì Created: loan_affordability (loan amount relative to total income over term)\")\n",
    "\n",
    "# 6. Stability Score\n",
    "df_scaled['stability_score'] = (\n",
    "    (df_scaled['employment_duration_months'] > 12).astype(int) * 0.4 +\n",
    "    (df_scaled['num_delinquencies'] == 0).astype(int) * 0.3 +\n",
    "    (df_scaled['payment_history_months'] > 24).astype(int) * 0.3\n",
    ")\n",
    "print(\"‚úì Created: stability_score (employment + payment reliability)\")\n",
    "\n",
    "# 7. Age-Income Interaction\n",
    "df_scaled['age_income_interaction'] = df_scaled['age'] * np.log1p(df_scaled['annual_income'])\n",
    "print(\"‚úì Created: age_income_interaction\")\n",
    "\n",
    "# 8. Total Debt Burden\n",
    "df_scaled['total_debt_burden'] = (df_scaled['existing_debt'] + df_scaled['loan_amount']) / (df_scaled['annual_income'] + 1)\n",
    "print(\"‚úì Created: total_debt_burden (existing + new debt / income)\")\n",
    "\n",
    "# 9. Credit Account Diversity\n",
    "df_scaled['account_diversity_score'] = np.minimum(df_scaled['num_credit_accounts'] / 10, 1.0)\n",
    "print(\"‚úì Created: account_diversity_score (normalized credit accounts)\")\n",
    "\n",
    "# 10. Risk Flags (binary indicators)\n",
    "df_scaled['high_dti_flag'] = (df_scaled['debt_to_income_ratio'] > 0.5).astype(int)\n",
    "df_scaled['high_utilization_flag'] = (df_scaled['credit_utilization'] > 0.8).astype(int)\n",
    "df_scaled['low_credit_flag'] = (df_scaled['credit_score'] < 600).astype(int)\n",
    "df_scaled['unemployed_flag'] = (df_scaled['employment_status_Unemployed'] == 1).astype(int) if 'employment_status_Unemployed' in df_scaled.columns else 0\n",
    "df_scaled['has_delinquencies_flag'] = (df_scaled['num_delinquencies'] > 0).astype(int)\n",
    "\n",
    "print(\"‚úì Created: 5 risk flag features\")\n",
    "\n",
    "print(f\"\\n‚úÖ Feature engineering completed!\")\n",
    "print(f\"Total features: {len(df_scaled.columns)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Save Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print_section_header(\"SAVE CLEAN DATASET\")\n",
    "\n",
    "# Define output path\n",
    "output_path = Path.cwd().parent / 'data' / 'cleaned' / 'Atuhaire.csv'\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the cleaned and processed dataset\n",
    "save_dataset(df_scaled, str(output_path), index=False)\n",
    "\n",
    "print(f\"\\nüìä Final Dataset Summary:\")\n",
    "print(f\"   Rows: {len(df_scaled):,}\")\n",
    "print(f\"   Columns: {len(df_scaled.columns)}\")\n",
    "print(f\"   Missing Values: {df_scaled.isnull().sum().sum()}\")\n",
    "print(f\"   File: {output_path}\")\n",
    "\n",
    "# Display sample of final dataset\n",
    "print(\"\\nSample of cleaned dataset:\")\n",
    "df_scaled.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section5'></a>\n",
    "## 5. MILESTONE ONE Summary\n",
    "\n",
    "### Achievements\n",
    "\n",
    "‚úÖ **Section 1: CRISP-DM Framework (8 marks)**\n",
    "- Defined clear research hypotheses (null and alternative)\n",
    "- Identified dependent variable (default_status) and independent variables\n",
    "- Established population, sample, and observational study design\n",
    "- Mapped complete CRISP-DM lifecycle\n",
    "\n",
    "‚úÖ **Section 2: Data Acquisition (8 marks)**\n",
    "- Generated comprehensive credit scoring dataset (40,000 records)\n",
    "- Documented data structure, volume, and characteristics\n",
    "- Identified data inconsistencies and quality issues\n",
    "- Assessed privacy risks for sensitive financial data\n",
    "\n",
    "‚úÖ **Section 3: Privacy & Compliance**\n",
    "- Demonstrated Uganda Data Protection Act compliance\n",
    "- Applied GDPR principles:\n",
    "  - Data minimization (only necessary features)\n",
    "  - De-identification (pseudonymization, generalization)\n",
    "  - Consent framework design\n",
    "  - Storage and access governance policies\n",
    "\n",
    "‚úÖ **Section 4: Data Preparation & Feature Engineering (14 marks)**\n",
    "- Handled missing values using appropriate imputation strategies\n",
    "- Detected and treated outliers (IQR capping method)\n",
    "- Applied data transformations (log, square root)\n",
    "- Performed comprehensive EDA with visualizations\n",
    "- Generated key insights about default patterns\n",
    "- Encoded categorical variables (label and one-hot encoding)\n",
    "- Scaled numerical features (StandardScaler)\n",
    "- Created 15+ domain-specific engineered features\n",
    "- Saved clean dataset as 'Atuhaire.csv'\n",
    "\n",
    "### Key Deliverables\n",
    "\n",
    "1. ‚úÖ Comprehensive problem definition with hypotheses\n",
    "2. ‚úÖ Clean dataset: `Atuhaire.csv`\n",
    "3. ‚úÖ EDA visualizations and insights\n",
    "4. ‚úÖ Privacy compliance documentation\n",
    "5. ‚úÖ Feature engineering pipeline\n",
    "\n",
    "### Next Steps (MILESTONE TWO)\n",
    "\n",
    "- Model selection and justification\n",
    "- Model training with MLflow tracking\n",
    "- Hyperparameter tuning\n",
    "- Model explainability (SHAP/LIME)\n",
    "- Fairness analysis\n",
    "- Deployment and monitoring\n",
    "\n",
    "---\n",
    "\n",
    "**End of MILESTONE ONE**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
