# Exam Completion Roadmap

**Student**: Atuhaire (B35093)  
**Submission Deadline**: December 14, 2025  
**Current Date**: December 9, 2025  
**Time Remaining**: 5 days

---

## âœ… COMPLETED (Estimated: 8-10 hours of work)

### âœ“ Project Foundation
- Git repository with 2 commits
- Professional project structure
- All utility modules created
- Documentation (README, QuickStart)

### âœ“ MILESTONE ONE - 100% Complete (30/30 marks potential)
- CRISP-DM framework fully documented
- Dataset created (40,000 records)
- Privacy compliance demonstrated
- Clean dataset: **Atuhaire.csv** âœ“
- Comprehensive EDA with 10+ insights
- 15+ engineered features
- 2 complete Jupyter notebooks

### âœ“ MILESTONE TWO - 40% Complete (started, 16/40 marks potential so far)
- Model selection justified (6 models)
- Data split completed
- SMOTE for class imbalance
- FastAPI deployment code
- Dockerfile created
- Deployment notebook started

---

## ðŸ”„ TO COMPLETE - Step by Step

### STEP 1: Complete Model Training (2-3 hours)

**Notebook**: `02_model_development.ipynb`

Add these sections to the notebook:

```python
# Section 2: Model Training & MLflow Tracking

## 2.1 Setup MLflow
import mlflow
mlflow.set_experiment("credit_scoring_models")

## 2.2 Train Logistic Regression
with mlflow.start_run(run_name="logistic_regression"):
    model_lr = LogisticRegression(max_iter=1000, class_weight='balanced')
    model_lr.fit(X_train_balanced,y_train_balanced)
    
    # Predict and log metrics
    y_pred = model_lr.predict(X_val)
    y_prob = model_lr.predict_proba(X_val)[:, 1]
    
    metrics = {
        'accuracy': accuracy_score(y_val, y_pred),
        'precision': precision_score(y_val, y_pred),
        'recall': recall_score(y_val, y_pred),
        'f1': f1_score(y_val, y_pred),
        'roc_auc': roc_auc_score(y_val, y_prob)
    }
    
    mlflow.log_params(model_lr.get_params())
    mlflow.log_metrics(metrics)
    mlflow.sklearn.log_model(model_lr, "model")

## 2.3 Train Random Forest
# Repeat for Random Forest with hyperparameter tuning

## 2.4 Train XGBoost
# Repeat for XGBoost (best model typically)

## 2.5 Train LightGBM
# Repeat for LightGBM
```

**Deliverable**: Trained models with MLflow logs

### STEP 2: Add Model Explainability (1-2 hours)

```python
# Section 3: Model Explainability

## 3.1 SHAP Values
import shap
explainer = shap.TreeExplainer(best_model)  # XGBoost or RandomForest
shap_values = explainer.shap_values(X_test)

# Summary plot
shap.summary_plot(shap_values, X_test)

# Force plot for single prediction
shap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:])

## 3.2 Feature Importance
importance_df = pd.DataFrame({
    'feature': feature_cols,
    'importance': best_model.feature_importances_
}).sort_values('importance', ascending=False)

plt.figure(figsize=(10, 8))
plt.barh(importance_df['feature'][:15], importance_df['importance'][:15])
plt.title('Top 15 Feature Importances')
plt.show()
```

**Deliverable**: SHAP visualizations and feature importance analysis

### STEP 3: Comprehensive Model Evaluation (1 hour)

```python
# Section 4: Model Evaluation

## 4.1 Test Set Performance
y_test_pred = best_model.predict(X_test)
y_test_prob = best_model.predict_proba(X_test)[:, 1]

print(classification_report(y_test, y_test_pred))

## 4.2 Confusion Matrix
cm = confusion_matrix(y_test, y_test_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix - Test Set')
plt.show()

## 4.3 ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_test_prob)
plt.plot(fpr, tpr, label=f'AUC = {roc_auc_score(y_test, y_test_prob):.3f}')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

## 4.4 Precision-Recall Curve
precision, recall, thresholds = precision_recall_curve(y_test, y_test_prob)
plt.plot(recall, precision)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.show()

## 4.5 Cross-Validation
cv_scores = cross_val_score(best_model, X_train_balanced, y_train_balanced, 
                           cv=5, scoring='roc_auc')
print(f"CV AUC Scores: {cv_scores}")
print(f"Mean CV AUC: {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})")
```

**Deliverable**: Complete evaluation metrics and visualizations

### STEP 4: Fairness Analysis (1-2 hours)

```python
# Section 5: Fairness Analysis

## 5.1 Load Sensitive Attributes
# Use the sensitive_test dataset we created earlier

## 5.2 Demographic Parity
from fairlearn.metrics import demographic_parity_difference, selection_rate

for sensitive_attr in ['gender', 'age_group']:
    if sensitive_attr in sensitive_test.columns:
        dpd = demographic_parity_difference(
            y_test, y_test_pred, 
            sensitive_features=sensitive_test[sensitive_attr]
        )
        print(f"Demographic Parity Difference ({sensitive_attr}): {dpd:.4f}")

## 5.3 Disparate Impact Ratio
def disparate_impact_ratio(y_pred, sensitive_feature):
    """Calculate 80% rule compliance"""
    groups = sensitive_feature.unique()
    rates = {}
    for group in groups:
        mask = sensitive_feature == group
        rates[group] = y_pred[mask].mean()
    
    min_rate = min(rates.values())
    max_rate = max(rates.values())
    return min_rate / max_rate if max_rate > 0 else 0

## 5.4 Fairness Visualization
# Plot approval rates by group
```

**Deliverable**: Fairness metrics demonstrating compliance

### STEP 5: Save Best Model (15 minutes)

```python
# Save best model
import joblib

model_path = Path.cwd().parent / 'models'
model_path.mkdir(exist_ok=True)

joblib.dump(best_model, model_path / 'best_model.pkl')
joblib.dump(feature_cols, model_path / 'feature_columns.pkl')

print(f"âœ… Model saved to {model_path}")
```

**Deliverable**: Serialized model for deployment

### STEP 6: Test API Deployment (30 minutes)

```bash
# Terminal 1: Start API
cd "d:\Projects\data science exam"
python src/deployment/api.py

# Terminal 2: Test API
# Use the example from QUICKSTART.md

# OR use browser: http://localhost:8000/docs
```

**Deliverable**: Screenshots of API running and Swagger UI

### STEP 7: Build Docker Image(30 minutes)

```bash
# Build image
docker build -t credit-scoring-api:latest .

# Run container
docker run -d -p 8000:8000 credit-scoring-api:latest

# Test
curl http://localhost:8000/health

# Screenshot
docker ps
docker images
```

**Deliverable**: Screenshots of Docker running

### STEP 8: Create Final Report (3-4 hours)

**File**: `reports/Final_Report.pdf`

**Structure** (4-5 pages):

1. **Executive Summary** (0.5 pages)
   - Project overview
   - Key findings
   - Recommendations

2. **Problem Statement & Methodology** (1 page)
   - Business problem
   - CRISP-DM approach
   - Hypotheses

3. **Data & Privacy** (0.5 pages)
   - Dataset description
   - Privacy compliance (GDPR, Uganda DPA)
   - Data workflows diagram

4. **Model Development** (1.5 pages)
   - Models evaluated
   - Best model selection
   - Performance metrics table
   - SHAP visualizations

5. **Fairness & Ethics** (0.5 pages)
   - Fairness metrics
   - Bias mitigation
   - Ethical considerations

6. **Deployment** (0.5 pages)
   - Architecture diagram
   - API screenshots
   - Docker images screenshot
   - MLflow screenshot

7. **Results & Impact** (0.5 pages)
   - Business impact
   - Performance summary
   -Key insights

8. **Limitations & Future Work** (0.5 pages)
   - Current limitations
   - Recommended improvements
   - Future enhancements

**Tools to create PDF**:
- Microsoft Word â†’ Export as PDF
- Google Docs â†’ Download as PDF
- LaTeX (Overleaf)
- Jupyter Notebook â†’ PDF (via nbconvert)

### STEP 9: Create Presentation (2-3 hours)

**File**: `reports/Presentation.pptx`

**15 Slides Maximum**:

1. Title Slide (name, access number, course, date)
2. Agenda
3. Problem Statement
4. CRISP-DM Methodology
5. Dataset Overview
6. Privacy & Compliance
7. EDA Key Insights
8. Feature Engineering
9. Model Selection
10. Model Performance
11. Fairness Analysis
12. Deployment Architecture
13. MLflow & Monitoring
14. Business Impact & Recommendations
15. Q&A / Thank You

**Tools**:
- Microsoft PowerPoint
- Google Slides
- Canva

### STEP 10: Self-Assessment (15 minutes)

**File**: `reports/Self_Assessment.txt`

**150 words maximum** covering:
- What you accomplished
- Challenges faced
- Key learnings
- Personal contribution

Example:
```
Throughout this project, I successfully implemented a complete data science
lifecycle for credit scoring, demonstrating proficiency in CRISP-DM methodology,
privacy compliance, and ML operations. I created a comprehensive dataset of
40,000 loan applications, performed extensive EDA, and engineered 15+ domain-
specific features. The main challenges included addressing class imbalance
through SMOTE and ensuring algorithmic fairness across demographic groups.

I trained and evaluated 6 models, with XGBoost achieving 82% accuracy and 
0.85 AUC-ROC. SHAP analysis revealed credit score and debt-to-income ratio
as top predictors. I successfully deployed a FastAPI application with Docker
containerization and implemented MLflow for experiment tracking.

Key learnings include the importance of fairness-aware ML in financial 
applications, the value of comprehensive privacy compliance frameworks, and 
the practical application of model explainability techniques. This project
enhanced my skills in end-to-end ML system development and deployment.

[Word count: 150]
```

### STEP 11: Git Final Commits (15 minutes)

```bash
# Add all final changes
git add -A

# Commit final report
git commit -m "Add final report and presentation"

# Commit self-assessment
git commit -m "Add self-assessment and documentation"

# View complete commit history
git log --oneline --graph --all
```

**Deliverable**: Complete Git history for individual contribution proof

### STEP 12: Package for Submission (30 minutes)

1. **Create submission folder**:
```bash
cd "d:\Projects\data science exam"
mkdir submission
```

2. **Copy required files**:
```
submission/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_acquisition_wrangling.ipynb
â”‚   â”œâ”€â”€ 01_part2_data_preparation.ipynb
â”‚   â””â”€â”€ 02_model_development.ipynb
â”œâ”€â”€ data/cleaned/
â”‚   â””â”€â”€ Atuhaire.csv
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ Final_Report.pdf
â”‚   â”œâ”€â”€ Presentation.pptx
â”‚   â””â”€â”€ Self_Assessment.txt
â”œâ”€â”€ src/
â”œâ”€â”€ models/
â”œâ”€â”€ git_logs.txt (git log --all --graph)
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

3. **Create zip file**:
```bash
# Windows PowerShell:
Compress-Archive -Path submission\* -DestinationPath "B35093_MSDS1_Data Science Lifecycle.zip"
```

4. **Verify zip contents**:
- Check file names (no personal names)
- Verify all notebooks run
- Ensure PDF is properly formatted
- Check all screenshots included

**Deliverable**: `B35093_MSDS1_Data Science Lifecycle.zip`

### STEP 13: Final Review Checklist (30 minutes)

- [ ] All notebooks execute without errors
- [ ] Atuhaire.csv file present
- [ ] Models saved in models/ folder
- [ ] API code functional
- [ ] Dockerfile builds successfully
- [ ] Final report 4-5 pages
- [ ] Presentation max 15 slides
- [ ] Self-assessment 150 words
- [ ] Git commit logs included
- [ ] No personal name in files
- [ ] Zip file correctly named
- [ ] File size< upload limit

### STEP 14: Submit to Moodle (15 minutes)

1. Log in to UCU Moodle
2. Navigate to DSC8201 course
3. Find submission portal
4. Upload zip file
5. Verify upload successful
6. Download submitted file to confirm
7. Keep backup copy

---

## â±ï¸ Time Estimate Summary

| Task | Estimated Time |
|------|----------------|
| Complete model training | 2-3 hours |
| Add explainability | 1-2 hours |
| Model evaluation | 1 hour |
| Fairness analysis | 1-2 hours |
| Save models | 15 min |
| Test API | 30 min |
| Docker build & test | 30 min |
| Final report | 3-4 hours |
| Presentation | 2-3 hours |
| Self-assessment | 15 min |
| Git commits | 15 min |
| Package submission | 30 min |
| Final review | 30 min |
| Submit | 15 min |
| **TOTAL** | **13-18 hours** |

**Recommended Schedule** (5 days remaining):

- **Day 1 (Dec 9)**: Complete model training & explainability (4 hours)
- **Day 2 (Dec 10)**: Evaluation & fairness analysis (3 hours)
- **Day 3 (Dec 11)**: Final report writing (4 hours)
- **Day 4 (Dec 12)**: Presentation creation (3 hours)
- **Day 5 (Dec 13)**: Review, package, submit (3 hours)
- **Buffer**: Dec 14 (deadline day) for any issues

---

## ðŸŽ¯ Success Criteria Reminder

### PART A (30 marks):
- âœ… CRISP-DM documented
- âœ… Data acquired
- âœ… Privacy compliance
- âœ… Clean dataset created

### PART B (40 marks):
- âœ… Models selected (partially)
- â³ Models trained (in progress)
- â³ Evaluation complete (to do)
- â³ Deployment done (code ready, needs testing)

### PART C (30 marks):
- âŒ Report (to do)
- âŒ Presentation (to do)
- âŒ Self-assessment (to do)

---

**You're well on your way! Follow this roadmap step-by-step. Good luck! ðŸš€**
